---
title: "花书 阅读笔记"
date: 2020-10-05T19:34:16+08:00
categories: ["Alchemy Of CV"]
tags: ["note", "book", "deep learning", "cv"]
draft: false
---

# Chapter 1
硬编码知识体系 —— **知识库方法**（Cyc）  

从原始数据中提取模式 —— **机器学习**  
简单的机器学习算法的性能很大程度上以来于给定数据的**表示**  
很难知道该提取哪些特征  

解决途径之一：使用机器学习来发掘表示本身，而不仅仅把表示映射到输出 —— **表示学习**  （自编码器）  
**变差因素**的组合过于复杂  

**深度学习**将复杂映射分解为一系列嵌套的简单映射  

**联结主义**的中心思想：当网络将大量简单的计算单元连接在一起时可以实现智能行为。  
**分布式表示**：系统的每一个输入都应该由多个特征表示，并且每一个特征都应该参与到多个可能输入的表示。  

# Chapter 2
## 范数

范数是将向量映射到非负值的函数。  

范数满足下列性质：  
+ \\(f(x) = 0 \Rightarrow x = 0\\)
+ \\(f(x+y) \le f(x) + f(y)\\) （三角不等式）
+ \\(f(ax) = |a|f(x)\\)

当机器学习问题中**零和非零元素**之间的差异非常重要时，通常会使用 L1 范数。  

## 特征分解

可以将 A 看作沿方向 \\(v_{i}\\) 延展 \\(\lambda_{i}\\) 倍的空间。  

## 行列式

行列式的绝对值可以用来衡量矩阵参与矩阵乘法后空间扩大或缩小了多少。  

# Chapter 3
不确定性的三种可能来源：    
+ 被建模系统内在的随机性
+ 不完全观测
+ 不完全建模

概率直接与事件发生的频率相联系 —— **频率派概率**  
衡量信任度（确定性水平） —— **贝叶斯概率**  

**条件概率** —— 某个事件在其他事件发生**时**出现的概率  
**干预查询** —— 一个行动的后果，即当采用某个动作**后**会发生什么  

### 信息论
**基本想法**：一个不太可能的事情居然发生了，要比一个非常可能的事件发生，能提供更多的信息。  

一个事件 x = \\(x\\) 的自信息：  

$$ I(x) = -logP(x) $$

自信息只处理单个输出，可以用香农熵来对整个概率分布中的不确定性总量进行量化：  

$$ H(x) = E_{x \sim P}[I(x)] = -E_{x \sim P}[logP(x)] $$

也记做 \\(H(P)\\)。  

一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总和。接近确定性的分布具有较低的熵，反之较高。  

如果对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，可以用 KL 散度衡量两个分布的差异：  

$$ D_{KL}(P || Q) = E_{x \sim P}[log \frac{P(x)}{Q(x)}] = E_{x \sim P}[logP(x) - logQ(x)] $$

KL 散度是**非负**的，当且仅当 P 和 Q 在离散型变量情况下是相同分布，或在连续型变量情况下是“几乎处处”相同的。  

KL 散度**不对称**。  

交叉熵：  

$$ H(P, Q) = H(P) + D_{KL}(P || Q) = -E_{x \sim P}logQ(x) $$

# Chapter 5
对于**输入缺失分类**任务，学习算法学习**一组**函数。  

### 量化模型容量
VC维：分类器能够分类的训练样本的最大数目。  

训练误差和泛化误差之间差异的上界随着模型容量的增长而增长，但随着训练样本增多而下降。  

容量**任意高**的极端情况 —— **非参数**模型(如最近邻回归)  

从预先知道的真实分布 \\(p(x, y)\\) 预测而出现的误差(分布中仍然会有一些噪声)被称为**贝叶斯误差**。  

### 没有免费午餐定理与正则化
没有一个机器学习算法总是比其他的好，必须在**特定任务**上设计性能良好的机器学习算法 \\(\Rightarrow\\) 设置学习算法的**偏好** \\(\Rightarrow\\) 正则化    

正则化：修改学习算法，使其降低泛化误差而非训练误差。  
